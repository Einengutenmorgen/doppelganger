#!/usr/bin/env python3

import argparse
import json
import logging
import os
import sys
from datetime import datetime, UTC 
from datetime import timezone
from pathlib import Path
from typing import Dict, List
from dotenv import load_dotenv
import traceback2 as traceback
from itertools import islice


# Add project root to Python path
project_root = str(Path(__file__).parent.parent)
sys.path.append(project_root)

from research_case.analyzers.llm_client import LLMClient
from research_case.generators.conversation_generator import ConversationGenerator, ConversationPrompt

# Setup logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

def generate_responses_for_conversation(
    conversation_id: str,
    conversation_messages: List[Dict],
    persona: Dict,
    generator: ConversationGenerator,
    max_generations: int = 5
) -> List[Dict]:
    """
    Generate responses for a conversation thread up to max_generations.
    
    Args:
        conversation_id: Unique conversation identifier
        conversation_messages: List of messages in conversation
        persona: User's persona data
        generator: ConversationGenerator instance
        max_generations: Maximum number of responses to generate per conversation
        
    Returns:
        List of generated response records
    """
    generated_records = []
    timestamp = datetime.now(timezone.utc).isoformat()
    
    # Sort messages by timestamp to maintain conversation flow
    messages = sorted(conversation_messages, key=lambda x: x.get('created_at', ''))
    
    # Generate response for each message except the last one, up to max_generations
    generations_count = 0
    for i in range(len(messages) - 1):
        if generations_count >= max_generations:
            logger.info(f"Reached maximum generations ({max_generations}) for conversation {conversation_id}")
            break
            
        current_msg = messages[i]
        next_msg = messages[i + 1]
        
        # Only generate response if next message is from our target user
        if str(next_msg.get('original_user_id')) in persona:
            # Create prompt with conversation history up to current message
            prompt = ConversationPrompt(
                persona=persona[str(next_msg.get('original_user_id'))],
                conversation_history=messages[:i+1],
                parent_message=current_msg.get('full_text', '')
            )
            
            generated_text = generator.generate_response(prompt)
            
            record = {
                "conversation_id": conversation_id,
                "generation_id": f"{conversation_id}_gen_{i}",
                "original_message_id": next_msg.get('tweet_id'),
                "original_text": next_msg.get('full_text', ''),
                "original_timestamp": next_msg.get('created_at', ''),
                "parent_message_id": current_msg.get('tweet_id'),
                "parent_message": current_msg.get('full_text', ''),
                "generated_text": generated_text,
                "generation_timestamp": timestamp,
                "persona_writing_style": prompt.persona.get("writing_style", ""),
                "persona_tone": prompt.persona.get("tone", ""),
                "persona_topics": prompt.persona.get("topics", "")
            }
            
            generated_records.append(record)
            generations_count += 1
    
    return generated_records

def main():
    # Step 1: Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Generate synthetic conversation responses based on personas."
    )
    parser.add_argument(
        "--personas",
        type=str,
        required=True,
        help="Path to the JSON file containing user personas."
    )
    parser.add_argument(
        "--conversations",
        type=str,
        required=False,
        default='data/preprocessed/processed_conversations.json',
        help="Path to the JSON file containing conversation threads."
    )
    parser.add_argument(
        "--output",
        type=str,
        required=False,
        help="Path to save the generated responses. Defaults to '<personas>/generated_conversations.json'."
    )
    parser.add_argument(
        "--min-messages",
        type=int,
        default=5,
        help="Minimum number of messages in conversation to process (default: 5)."
    )
    parser.add_argument(
        "--replies-generations-per-conversation",
        type=int,
        default=1,
        help="Maximum number of responses to generate per conversation (default: 1)."
    )
    parser.add_argument(
        "--for-n-conversations",
        type=int,
        default=50,
        help="Maximum number of conversations that should be used (default: 50)."
    )
    args = parser.parse_args()

    # Step 2: Set up file paths
    personas_path = args.personas
    conversations_path = args.conversations
    output_path = args.output or os.path.join(
        os.path.dirname(personas_path), "generated_conversations.json"
    )

    # Validate input files
    for path in [personas_path, conversations_path]:
        if not os.path.exists(path):
            logger.error(f"Input file not found: {path}")
            exit(1)

    # Step 3: Load environment variables and initialize client
    load_dotenv()
    llm_client = LLMClient(api_key=os.getenv('OPENAI_API_KEY'))
    conversation_generator = ConversationGenerator(llm_client)

    # Step 4: Run generation
    try:
        logger.info("Starting conversation response generation...")
        
        # Load personas and conversations
        with open(personas_path, 'r') as f:
            personas = json.load(f)
        with open(conversations_path, 'r') as f:
            conversations = json.load(f)

        # Initialize results list for flat structure
        all_generated_records = []

        # Generate responses for each conversation
        for conv_id, messages in islice(conversations.items(), args.for_n_conversations):
            if len(messages) < args.min_messages:
                logger.debug(f"Skipping conversation {conv_id} - too few messages")
                continue
                
            logger.info(f"Processing conversation {conv_id}")
            
            # Generate responses for this conversation
            conv_records = generate_responses_for_conversation(
                conversation_id=conv_id,
                conversation_messages=messages,
                persona=personas,
                generator=conversation_generator,
                max_generations=args.replies_generations_per_conversation
            )
            
            all_generated_records.extend(conv_records)

        # Add generation run metadata
        output_data = {
            "metadata": {
                "generation_timestamp": datetime.now(timezone.utc).isoformat(),
                "conversations_processed": len(conversations),
                "total_responses_generated": len(all_generated_records),
                "min_messages": args.min_messages,
                "max_generations": args.replies_generations_per_conversation
            },
            "generated_responses": all_generated_records
        }

        # Save results
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=4)

        logger.info(f"Response generation completed. Generated {len(all_generated_records)} responses.")
        logger.info(f"Results saved to {output_path}")

    except Exception as e:
        logger.error("Failed to generate responses:")
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()
