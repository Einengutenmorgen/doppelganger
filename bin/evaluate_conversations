#!/usr/bin/env python3
"""Evaluate generated conversation responses."""

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List
from dotenv import load_dotenv
import traceback2 as traceback

# Add project root to Python path
project_root = str(Path(__file__).parent.parent)
sys.path.append(project_root)

from research_case.evaluator.conversation_pipeline import ConversationEvaluationPipeline
from research_case.evaluator.conversation_judge import ConversationJudge
from research_case.analyzers.llm_client import LLMClient

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_args() -> argparse.Namespace:
    """Set up command line arguments."""
    parser = argparse.ArgumentParser(
        description='Evaluate generated conversation responses.'
    )
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='Path to the JSON file containing generated responses'
    )
    parser.add_argument(
        '--conversations',
        type=str,
        required=True,
        help='Path to the JSON file containing original conversations'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=os.path.join(project_root, 'evaluation_results'),
        help='Directory to save evaluation results'
    )
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        default='INFO',
        help='Set the logging level'
    )
    
    return parser.parse_args()

def generate_report(results: dict) -> dict:
    """Generate a structured report from evaluation results."""
    report = {
        "timestamp": datetime.now().isoformat(),
        "total_responses_evaluated": results['metadata']['total_evaluated'],
        "metrics": {
            "rouge_scores": {},
            "similarity_scores": {},
            "conversation_metrics": {}
        }
    }
    
    agg = results.get('aggregate_metrics', {})
    
    # Add ROUGE scores
    if 'rouge_metrics' in agg:
        report["metrics"]["rouge_scores"] = {
            "rouge1_f1": agg['rouge_metrics'].get('rouge1', {}).get('fmeasure', {}).get('mean', 0),
            "rouge2_f1": agg['rouge_metrics'].get('rouge2', {}).get('fmeasure', {}).get('mean', 0),
            "rougeL_f1": agg['rouge_metrics'].get('rougeL', {}).get('fmeasure', {}).get('mean', 0)
        }
    
    # Add similarity scores
    if 'similarity_metrics' in agg:
        report["metrics"]["similarity_scores"] = {
            "mean": agg['similarity_metrics'].get('mean', 0)
        }
    
    # Add conversation-specific metrics
    if 'conversation_metrics' in agg:
        conv_metrics = agg['conversation_metrics']
        report["metrics"]["conversation_metrics"] = {
            "response_appropriateness": conv_metrics.get('response_appropriateness', {}).get('mean', 0),
            "persona_consistency": conv_metrics.get('persona_consistency', {}).get('mean', 0),
            "context_awareness": conv_metrics.get('context_awareness', {}).get('mean', 0),
            "natural_flow_ratio": conv_metrics.get('natural_flow_ratio', 0)
        }
    
    return report

def save_report(report: dict, base_path: str) -> str:
    """Save report to a JSON file."""
    # Create reports directory if it doesn't exist
    reports_dir = os.path.join(os.path.dirname(base_path), "reports")
    os.makedirs(reports_dir, exist_ok=True)
    
    # Generate filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"conversation_evaluation_report_{timestamp}.json"
    report_path = os.path.join(reports_dir, filename)
    
    # Save report
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    return report_path

def print_summary(results: dict):
    """Print summary of evaluation results."""
    print("\nConversation Evaluation Summary:")
    print(f"Total responses evaluated: {results['metadata']['total_evaluated']}")
    
    if results.get('aggregate_metrics'):
        agg = results['aggregate_metrics']
        
        print("\nAverage Scores:")
        
        # ROUGE scores
        if 'rouge_metrics' in agg and 'rouge1' in agg['rouge_metrics']:
            rouge1 = agg['rouge_metrics']['rouge1']
            print(f"ROUGE-1 F1: {rouge1['fmeasure']['mean']:.3f}")
        
        # Similarity scores
        if 'similarity_metrics' in agg:
            sim = agg['similarity_metrics']
            print(f"Semantic Similarity: {sim['mean']:.3f}")
        
        # Conversation metrics
        if 'conversation_metrics' in agg:
            conv = agg['conversation_metrics']
            print(f"Response Appropriateness: {conv['response_appropriateness']['mean']:.1f}/10")
            print(f"Persona Consistency: {conv['persona_consistency']['mean']:.1f}/10")
            print(f"Context Awareness: {conv['context_awareness']['mean']:.1f}/10")
            print(f"Natural Flow Ratio: {conv['natural_flow_ratio']:.2%}")

def main():
    """Main execution function."""
    args = setup_args()
    # Set logging level
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # Load environment variables
    load_dotenv()
    
    # Validate OpenAI API key
    if not os.getenv('OPENAI_API_KEY'):
        logger.error("OPENAI_API_KEY environment variable is required")
        sys.exit(1)
    
    try:
        # Load generated responses and original conversations
        logger.info(f"Loading generated responses from {args.input}")
        with open(args.input, 'r') as f:
            generated_data = json.load(f)
            
        logger.info(f"Loading original conversations from {args.conversations}")
        with open(args.conversations, 'r') as f:
            conversation_data = json.load(f)
            
        # Add conversation data to generated data for context
        generated_data['conversation_data'] = conversation_data
        
        # Initialize pipeline
        pipeline = ConversationEvaluationPipeline()
        
        # Run evaluation
        logger.info("Starting evaluation pipeline")
        results = pipeline.evaluate_batch(generated_data)
        
        # Save full results
        output_path = os.path.join(
            args.output_dir, 
            f"conversation_eval_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        os.makedirs(args.output_dir, exist_ok=True)
        pipeline.save_results(results, output_path)
        
        # Generate and save report
        report = generate_report(results)
        report_path = save_report(report, args.input)
        logger.info(f"Evaluation report saved to {report_path}")
        
        # Print summary
        print_summary(results)
        
    except FileNotFoundError:
        logger.error(f"Input file not found: {args.input}")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in input file: {args.input}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error running evaluation: {e}")
        logger.debug("Traceback:", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()