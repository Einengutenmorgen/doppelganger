#!/usr/bin/env python3
"""Evaluate generated social media posts."""

import argparse
import json
import logging
import os
import sys
from datetime import datetime

from research_case.evaluator.llm_judge import LLMJudge



def setup_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description='Evaluate generated social media posts.')
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='Path to the JSON file containing generated posts'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        help='Directory to save evaluation results'
    )
    parser.add_argument(
        '--llm-client',
        choices=['openai', 'gemini'],
        default='gemini',
        help='Choose which LLM client to use for evaluation'
    )
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        default='INFO',
        help='Set the logging level'
    )
    return parser.parse_args()

args = setup_args()

    
# Setup logging
logging.basicConfig(
    level=getattr(logging, args.log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
    


def save_results(results: dict, output_path: str):
    """Save evaluation results to a JSON file."""
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    logger.info(f"Evaluation results saved to {output_path}")

def main():
    """Main execution function."""
    print('Starting Eval Process ....')
    
    try:
    
        # Load environment variables
        logger.info('loading data ... ')
        
        with open(args.input, 'r') as f:
            data = json.load(f)
            #logger.debug(f"Full data structure: {json.dumps(data, indent=2)}")
                
        # Initialize the LLM judge
        judge = LLMJudge(client_type=args.llm_client)
        logger.info("Initialized LLM judge")
        
        results = []
        # Process each post
        for post in data['generated_posts']:
            try:
                evaluation = judge.evaluate_post(
                    original_post=post['original_text'],
                    generated_post=post['generated_text'],
                    persona={k.replace('persona_', ''): v for k, v in post.items() if k.startswith('persona_')},
                    stimulus=post['stimulus']
                )
                
                results.append({
                    'post_id': post['generation_id'],
                    'evaluation': evaluation
                })
                
            except Exception as e:
                logger.error(f"Error evaluating post {post.get('generation_id')}: {e}")
        
        # Save results
        output_filename = f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        output_dir = args.output_dir or os.path.dirname(args.input)
        output_path = os.path.join(output_dir, output_filename)
        
        save_results({
            'metadata': {
                'total_evaluated': len(results),
                'timestamp': datetime.now().isoformat(),
                'client_type': args.llm_client
            },
            'results': results
        }, output_path)
        
        print(f"\nEvaluation complete! Results saved to: {output_path}")
        print(f"Total posts evaluated: {len(results)}")
        
    except FileNotFoundError:
        logger.error(f"Input file not found: {args.input}")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in input file: {args.input}")
        sys.exit(1)
    except Exception as e:
        logger.debug("Traceback:", exc_info=True)
        logger.error(f"Error running evaluation: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()