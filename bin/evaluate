#!/usr/bin/env python3
"""Evaluate generated social media posts."""

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

print("Starting script...")

# Setup logging first
logging.basicConfig(
    level=logging.DEBUG,  
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

print("Loading environment variables...")
from dotenv import load_dotenv
load_dotenv()

print("Importing evaluation pipeline...")
from research_case.evaluator.pipeline import EvaluationPipeline

# Add the project root directory to Python path
# ROOT_DIR = Path(__file__).resolve().parent.parent
# sys.path.append(str(ROOT_DIR))


def setup_args() -> argparse.Namespace:
    """Set up command line arguments."""
    parser = argparse.ArgumentParser(
        description='Evaluate generated social media posts.'
    )
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='Path to the JSON file containing generated posts'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=os.path.join(os.path.dirname(__file__), '..', 'evaluation_results'),
        help='Directory to save evaluation results, Defaults to input directory'
    )
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        default='INFO',
        help='Set the logging level'
    )
    
    return parser.parse_args()

def generate_report(results: dict) -> dict:
    """Generate a structured report from evaluation results."""
    report = {
        "timestamp": datetime.now().isoformat(),
        "total_posts_evaluated": results['metadata']['total_evaluated'],
        "metrics": {
            "rouge_scores": {},
            "similarity_scores": {},
            "llm_evaluation": {}
        }
    }
    
    agg = results.get('aggregate_metrics', {})
    
    # Add ROUGE scores
    if 'rouge' in agg and 'rouge1' in agg['rouge']:
        report["metrics"]["rouge_scores"] = {
            "rouge1_f1": agg['rouge']['rouge1']['fmeasure']['mean']
        }
    
    # Add similarity scores
    if 'similarity_scores' in agg:
        report["metrics"]["similarity_scores"] = {
            "mean": agg['similarity_scores']['mean']
        }
    
    # Add LLM evaluation scores
    if 'llm_evaluation' in agg:
        report["metrics"]["llm_evaluation"] = {
            "authenticity": agg['llm_evaluation']['authenticity']['mean'],
            "style_consistency": agg['llm_evaluation']['style_consistency']['mean']
        }
    
    return report

def save_report(report: dict, base_path: str):
    """Save report to a JSON file."""
    # Create reports directory if it doesn't exist
    reports_dir = os.path.join(os.path.dirname(base_path), "reports")
    os.makedirs(reports_dir, exist_ok=True)
    
    # Generate filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"evaluation_report_{timestamp}.json"
    report_path = os.path.join(reports_dir, filename)
    
    # Save report
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    return report_path

def print_summary(results: dict):
    """Print summary of evaluation results."""
    print("\nEvaluation Summary:")
    print(f"Total posts evaluated: {results['metadata']['total_evaluated']}")
    if results.get('aggregate_metrics'):
        agg = results['aggregate_metrics']
        print("\nAverage Scores:")
        if 'rouge' in agg and 'rouge1' in agg['rouge']:
            rouge1 = agg['rouge']['rouge1']
            print(f"ROUGE-1 F1: {rouge1['fmeasure']['mean']:.3f}")
        if 'similarity_scores' in agg:
            sim = agg['similarity_scores']
            print(f"Semantic Similarity: {sim['mean']:.3f}")
        if 'llm_evaluation' in agg:
            llm = agg['llm_evaluation']
            print(f"Authenticity: {llm['authenticity']['mean']:.1f}/10")
            print(f"Style Consistency: {llm['style_consistency']['mean']:.1f}/10")

def main():
    """Main execution function."""
    print('Starting Eval Process ....')
    args = setup_args()
    # Set logging level
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # Load environment variables
    logger.info('loaded env variabels')
    
    try:
        # Load generated posts
        logger.info(f"Loading generated posts from {args.input}")
        with open(args.input, 'r') as f:
            data = json.load(f)
        
        # Initialize pipeline
        pipeline = EvaluationPipeline()
        
        # Run evaluation
        logger.info("Starting evaluation pipeline")
        results = pipeline.evaluate_batch(data)
        
        # Save full results
        output_path = os.path.join(
            os.path.dirname(args.input), "eval_results.json"
        )
        pipeline.save_results(results, output_path)
        
        # Generate and save report
        report = generate_report(results)
        report_path = save_report(report, args.input)
        logger.info(f"Evaluation report saved to {report_path}")
        
        # Print summary
        print_summary(results)
        
    except FileNotFoundError:
        logger.error(f"Input file not found: {args.input}")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in input file: {args.input}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error running evaluation: {e}")
        logger.debug("Traceback:", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    print('.... loading')
    main()