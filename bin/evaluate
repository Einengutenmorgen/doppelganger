#!/usr/bin/env python3
"""Evaluate generated social media posts."""

#import io
import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

# Add project root to Python path
project_root = str(Path(__file__).parent.parent)
sys.path.append(project_root)

from dotenv import load_dotenv
from research_case.evaluator import EvaluationPipeline

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_args() -> argparse.Namespace:
    """Set up command line arguments."""
    parser = argparse.ArgumentParser(
        description='Evaluate generated social media posts.'
    )
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='Path to the JSON file containing generated posts'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=os.path.join(project_root, 'evaluation_results'),
        help='Directory to save evaluation results'
    )
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        default='INFO',
        help='Set the logging level'
    )
    
    return parser.parse_args()

def print_summary(results: dict):
    """Print summary of evaluation results."""
    print("\nEvaluation Summary:")
    print(f"Total posts evaluated: {results['metadata']['total_evaluated']}")
    
    if results.get('aggregate_metrics'):
        agg = results['aggregate_metrics']
        
        print("\nAverage Scores:")
        if 'rouge' in agg and 'rouge1' in agg['rouge']:
            rouge1 = agg['rouge']['rouge1']
            print(f"ROUGE-1 F1: {rouge1['fmeasure']['mean']:.3f}")
            
        if 'similarity_scores' in agg:
            sim = agg['similarity_scores']
            print(f"Semantic Similarity: {sim['mean']:.3f}")
                        
        if 'llm_evaluation' in agg:
            llm = agg['llm_evaluation']
            print(f"Authenticity: {llm['authenticity']['mean']:.1f}/10")
            #print(f"Response Quality: {llm['response_quality']['mean']:.1f}/10")
            print(f"Style Consistency: {llm['style_consistency']['mean']:.1f}/10")

def main():
    """Main execution function."""
    args = setup_args()
    
    # Set logging level
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # Load environment variables
    load_dotenv()
    
    # Validate OpenAI API key
    if not os.getenv('OPENAI_API_KEY'):
        logger.error("OPENAI_API_KEY environment variable is required")
        sys.exit(1)
        
    try:
        # Load generated posts
        logger.info(f"Loading generated posts from {args.input}")
        with open(args.input, 'r') as f:
            data = json.load(f)
            
        # Initialize pipeline
        pipeline =  EvaluationPipeline()
        
        # Run evaluation
        logger.info("Starting evaluation pipeline")
        results = pipeline.evaluate_batch(data)
        
        output_path = os.path.join(
            os.path.dirname(args.input), "eval_results.json"
        )
        
        
        pipeline.save_results(results, output_path)
        logger.info(f"Evaluation completed. Results saved to {output_path}")
        
        # Print summary
        print_summary(results)
        
    except FileNotFoundError:
        logger.error(f"Input file not found: {args.input}")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in input file: {args.input}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error running evaluation: {e}")
        logger.debug("Traceback:", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()