#!/usr/bin/env python3

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List
from itertools import islice
from dotenv import load_dotenv
import traceback2 as traceback

# Add project root to Python path
project_root = str(Path(__file__).parent.parent)
sys.path.append(project_root)

from research_case.analyzers.llm_client import LLMClient
from research_case.generators.post_generator import PostGenerator, StimulusGenerator, GenerationPrompt


# Setup logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

def generate_posts_for_user(
    user_id: str,
    persona: Dict,
    original_posts: List[Dict],
    post_generator: PostGenerator,
    stimulus_generator: StimulusGenerator,
    posts_per_persona: int
) -> List[Dict]:
    """
    Generate posts for a single user and return in flat structure.
    
    Args:
        user_id: User identifier
        persona: User's persona data
        original_posts: List of original posts
        post_generator: PostGenerator instance
        stimulus_generator: StimulusGenerator instance
        posts_per_persona: Number of posts to generate
        
    Returns:
        List of generated post records
    """
    generated_records = []
    timestamp = datetime.utcnow().isoformat()
    
    for i in range(posts_per_persona):
        if i < len(original_posts):
            # Extract original post content
            original_post = original_posts[i].get('full_text', '')
            original_post_id = original_posts[i].get('tweet_id', '')
            original_timestamp = original_posts[i].get('created_at', '')
            
            # Create stimulus and generate new post
            stimulus = stimulus_generator.create_post_stimulus(original_post)
            prompt = GenerationPrompt(persona=persona, stimulus=stimulus)
            generated_text = post_generator.generate_post(prompt)
            
            # Create flat record structure
            record = {
                "user_id": user_id,
                "generation_id": f"{user_id}_gen_{i}",  # Unique identifier for generated post
                "original_post_id": original_post_id,
                "original_text": original_post,
                "original_timestamp": original_timestamp,
                "stimulus": stimulus,
                "generated_text": generated_text,
                "generation_timestamp": timestamp,
                # Include relevant persona characteristics for analysis
                "persona_writing_style": persona.get("writing_style", ""),
                "persona_tone": persona.get("tone", ""),
                "persona_topics": persona.get("topics", ""),
            }
            
            generated_records.append(record)
    
    return generated_records

def main():
    # Step 1: Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Generate synthetic social media posts based on personas."
    )
    parser.add_argument(
        "--personas",
        type=str,
        required=True,
        help="Path to the JSON file containing user personas."
    )
    parser.add_argument(
        "--posts",
        type=str,
        required=True,
        default=os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data/processed_data/processed_users.json'),
        help="Path to the JSON file containing original posts for stimulus generation."
    )
    parser.add_argument(
        "--output",
        type=str,
        required=False,
        help="Path to save the generated posts. Defaults to the persona dirctory '<input_dir>/generated_posts.json'."
    )
    parser.add_argument(
        "--posts-per-persona",
        type=int,
        default=5,
        help="Number of posts to generate per persona (default: 5)."
    )
    args = parser.parse_args()

    # Step 2: Set up file paths
    personas_path = args.personas
    posts_path = args.posts
    output_path = args.output or os.path.join(
        os.path.dirname(personas_path), "generated_posts.json"
    )

    # Validate input files
    for path in [personas_path, posts_path]:
        if not os.path.exists(path):
            logger.error(f"Input file not found: {path}")
            exit(1)

    # Step 3: Load environment variables and initialize clients
    load_dotenv()
    llm_client = LLMClient(api_key=os.getenv('OPENAI_API_KEY'))
    post_generator = PostGenerator(llm_client)
    stimulus_generator = StimulusGenerator(llm_client)

    # Step 4: Run generation
    try:
        logger.info("Starting post generation...")
        
        # Load personas and posts
        with open(personas_path, 'r') as f:
            personas = json.load(f)
        with open(posts_path, 'r') as f:
            original_posts = json.load(f)

        # Initialize results list for flat structure
        all_generated_records = []

        # Generate posts for each persona
        for user_id, persona in personas.items():
            logger.info(f"Generating posts for user {user_id}")
            
            # Get user's original posts
            user_posts = original_posts.get(user_id, [])
            if not user_posts:
                logger.warning(f"No original posts found for user {user_id}")
                continue
            
            # Generate posts for this user
            user_records = generate_posts_for_user(
                user_id=user_id,
                persona=persona,
                original_posts=user_posts,
                post_generator=post_generator,
                stimulus_generator=stimulus_generator,
                posts_per_persona=args.posts_per_persona
            )
            
            all_generated_records.extend(user_records)

        # Add generation run metadata
        output_data = {
            "metadata": {
                "generation_timestamp": datetime.utcnow().isoformat(),
                "num_users": len(personas),
                "posts_per_persona": args.posts_per_persona,
                "total_posts_generated": len(all_generated_records)
            },
            "generated_posts": all_generated_records
        }

        # Save results
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=4)

        logger.info(f"Post generation completed. Generated {len(all_generated_records)} posts.")
        logger.info(f"Results saved to {output_path}")

    except Exception as e:
        logger.error("Failed to generate posts:")
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()