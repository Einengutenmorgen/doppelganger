{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 8000 records from CSV file\n",
      "Matched ROUGE scores for 8000 of 8000 records\n",
      "Matched LLM evaluations for 8000 of 8000 records\n",
      "\n",
      "Data Summary:\n",
      "Total records: 8000\n",
      "Column completion rates:\n",
      "  user_id: 8000/8000 records (100.0%)\n",
      "  generation_id: 8000/8000 records (100.0%)\n",
      "  original_post_id: 8000/8000 records (100.0%)\n",
      "  original_text: 8000/8000 records (100.0%)\n",
      "  stimulus: 8000/8000 records (100.0%)\n",
      "  generation_index: 8000/8000 records (100.0%)\n",
      "  generated_text: 8000/8000 records (100.0%)\n",
      "  similarity_scores: 8000/8000 records (100.0%)\n",
      "  rouge1_fmeasure: 8000/8000 records (100.0%)\n",
      "  rouge1_recall: 8000/8000 records (100.0%)\n",
      "  rouge1_precision: 8000/8000 records (100.0%)\n",
      "  rouge2_fmeasure: 8000/8000 records (100.0%)\n",
      "  rouge2_recall: 8000/8000 records (100.0%)\n",
      "  rouge2_precision: 8000/8000 records (100.0%)\n",
      "  rougel_fmeasure: 8000/8000 records (100.0%)\n",
      "  rougel_recall: 8000/8000 records (100.0%)\n",
      "  rougel_precision: 8000/8000 records (100.0%)\n",
      "  llm_evaluation_authenticity: 8000/8000 records (100.0%)\n",
      "  llm_evaluation_style_consistency: 8000/8000 records (100.0%)\n",
      "  llm_evaluation_matching_intent: 8000/8000 records (100.0%)\n",
      "  persona_general_decription: 8000/8000 records (100.0%)\n",
      "  persona_brevity_style: 8000/8000 records (100.0%)\n",
      "  persona_language_formality: 8000/8000 records (100.0%)\n",
      "  persona_narrative_voice: 8000/8000 records (100.0%)\n",
      "  persona_vocabulary_range: 8000/8000 records (100.0%)\n",
      "  persona_punctuation_style: 8000/8000 records (100.0%)\n",
      "  persona_controversy_handling: 8000/8000 records (100.0%)\n",
      "  persona_community_role: 8000/8000 records (100.0%)\n",
      "  persona_content_triggers: 8000/8000 records (100.0%)\n",
      "  persona_reaction_patterns: 8000/8000 records (100.0%)\n",
      "  persona_message_effectiveness: 8000/8000 records (100.0%)\n",
      "  persona_opinion_expression: 8000/8000 records (100.0%)\n",
      "  persona_emotional_expression: 8000/8000 records (100.0%)\n",
      "  persona_cognitive_patterns: 8000/8000 records (100.0%)\n",
      "  persona_social_orientation: 8000/8000 records (100.0%)\n",
      "  persona_conflict_approach: 8000/8000 records (100.0%)\n",
      "  persona_value_signals: 8000/8000 records (100.0%)\n",
      "  persona_identity_projection: 8000/8000 records (100.0%)\n",
      "  persona_belief_expression: 8000/8000 records (100.0%)\n",
      "  persona_stress_indicators: 8000/8000 records (100.0%)\n",
      "  persona_adaptability_signs: 8000/8000 records (100.0%)\n",
      "  persona_authenticity_markers: 8000/8000 records (100.0%)\n",
      "\n",
      "Successfully created combined CSV file at: /Users/christophhau/Desktop/Research_case/results/fine_tune03/combined_results.csv\n",
      "Total records processed: 8000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "# Define file paths\n",
    "eval_results_path = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/eval_results.json\"\n",
    "llm_eval_results_path = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/judge/evaluation_results_20250325_121256.json\"\n",
    "generated_posts_path = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/8k_gpt4o_rnd_shuffled_personas.csv\"\n",
    "output_path = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/combined_results.csv\"\n",
    "\n",
    "# Load the evaluation results\n",
    "with open(eval_results_path, 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Load the LLM evaluation results\n",
    "with open(llm_eval_results_path, 'r') as f:\n",
    "    llm_eval_data = json.load(f)\n",
    "\n",
    "# Load the generated posts data\n",
    "# Note: This is described as CSV but has JSON content in the example\n",
    "# We'll handle both possibilities\n",
    "try:\n",
    "    # Try loading as CSV first\n",
    "    generated_posts_df = pd.read_csv(generated_posts_path)\n",
    "    is_csv = True\n",
    "    print(f\"Successfully loaded {len(generated_posts_df)} records from CSV file\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load as CSV, trying JSON format: {str(e)}\")\n",
    "    try:\n",
    "        # If that fails, try loading as JSON\n",
    "        with open(generated_posts_path, 'r') as f:\n",
    "            generated_posts_data = json.load(f)\n",
    "        is_csv = False\n",
    "        print(f\"Successfully loaded JSON data with {len(generated_posts_data.get('generated_posts', []))} posts\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load generated posts data in either CSV or JSON format: {str(e)}\")\n",
    "\n",
    "# Process the ROUGE and similarity scores data\n",
    "rouge_data = {}\n",
    "for item in eval_data['individual_evaluations']:\n",
    "    gen_id = item['metadata']['generated_id']\n",
    "    \n",
    "    # Extract ROUGE scores\n",
    "    rouge_scores = item['rouge_scores']\n",
    "    similarity = item['similarity_scores']\n",
    "    \n",
    "    rouge_data[gen_id] = {\n",
    "        'similarity_scores': similarity,\n",
    "        'rouge1_fmeasure': rouge_scores['rouge1']['fmeasure'],\n",
    "        'rouge1_recall': rouge_scores['rouge1']['recall'],\n",
    "        'rouge1_precision': rouge_scores['rouge1']['precision'],\n",
    "        'rouge2_fmeasure': rouge_scores['rouge2']['fmeasure'],\n",
    "        'rouge2_recall': rouge_scores['rouge2']['recall'],\n",
    "        'rouge2_precision': rouge_scores['rouge2']['precision'],\n",
    "        'rougel_fmeasure': rouge_scores['rougeL']['fmeasure'],\n",
    "        'rougel_recall': rouge_scores['rougeL']['recall'],\n",
    "        'rougel_precision': rouge_scores['rougeL']['precision']\n",
    "    }\n",
    "\n",
    "# Process the LLM evaluation data\n",
    "llm_eval_dict = {}\n",
    "for item in llm_eval_data['results']:\n",
    "    post_id = item['post_id']\n",
    "    eval_data = item['evaluation']\n",
    "    \n",
    "    llm_eval_dict[post_id] = {\n",
    "        'llm_evaluation_authenticity': eval_data['authenticity']['score'],\n",
    "        'llm_evaluation_style_consistency': eval_data['style_consistency']['score'],\n",
    "        'llm_evaluation_matching_intent': eval_data['matching_intent']\n",
    "    }\n",
    "\n",
    "# Prepare persona columns for Boolean conversion\n",
    "persona_columns = [\n",
    "    'persona_general_decription', 'persona_brevity_style', 'persona_language_formality',\n",
    "    'persona_narrative_voice', 'persona_vocabulary_range', 'persona_punctuation_style',\n",
    "    'persona_controversy_handling', 'persona_community_role', 'persona_content_triggers',\n",
    "    'persona_reaction_patterns', 'persona_message_effectiveness', 'persona_opinion_expression',\n",
    "    'persona_emotional_expression', 'persona_cognitive_patterns', 'persona_social_orientation',\n",
    "    'persona_conflict_approach', 'persona_value_signals', 'persona_identity_projection',\n",
    "    'persona_belief_expression', 'persona_stress_indicators', 'persona_adaptability_signs',\n",
    "    'persona_authenticity_markers'\n",
    "]\n",
    "\n",
    "# Process the generated posts data and create the final DataFrame\n",
    "if is_csv:\n",
    "    # If the file was successfully loaded as CSV\n",
    "    final_df = generated_posts_df.copy()\n",
    "    \n",
    "    # Clean up the generated_text field if it's in JSON format\n",
    "    # Use a safer approach to handle potentially malformed JSON\n",
    "    def extract_post_text(text):\n",
    "        if not isinstance(text, str) or not text.startswith('{'):\n",
    "            return text\n",
    "            \n",
    "        # Try to extract post_text using regex instead of direct JSON parsing\n",
    "        import re\n",
    "        match = re.search(r'\"post_text\"\\s*:\\s*\"([^\"]*)', text)\n",
    "        if match:\n",
    "            # Return the captured content, handling potential escaping\n",
    "            return match.group(1).replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n",
    "        \n",
    "        # If regex approach fails, return the original text\n",
    "        return text\n",
    "        \n",
    "    final_df['generated_text'] = final_df['generated_text'].apply(extract_post_text)\n",
    "else:\n",
    "    # If the file was loaded as JSON\n",
    "    # Extract the generated posts into a DataFrame\n",
    "    posts_list = generated_posts_data['generated_posts']\n",
    "    final_df = pd.DataFrame(posts_list)\n",
    "    \n",
    "    # Clean up the generated_text field if it's in JSON format\n",
    "    # Use same safe approach as above\n",
    "    def extract_post_text(text):\n",
    "        if not isinstance(text, str) or not text.startswith('{'):\n",
    "            return text\n",
    "            \n",
    "        # Try to extract post_text using regex instead of direct JSON parsing\n",
    "        import re\n",
    "        match = re.search(r'\"post_text\"\\s*:\\s*\"([^\"]*)', text)\n",
    "        if match:\n",
    "            # Return the captured content, handling potential escaping\n",
    "            return match.group(1).replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n",
    "        \n",
    "        # If regex approach fails, return the original text\n",
    "        return text\n",
    "        \n",
    "    final_df['generated_text'] = final_df['generated_text'].apply(extract_post_text)\n",
    "\n",
    "# Add generation_index column if it doesn't exist\n",
    "if 'generation_index' not in final_df.columns:\n",
    "    final_df['generation_index'] = final_df['generation_id'].apply(\n",
    "        lambda x: int(x.split('_gen_')[1]) if '_gen_' in str(x) else 0\n",
    "    )\n",
    "\n",
    "# Add ROUGE and similarity scores to the DataFrame\n",
    "rouge_match_count = 0\n",
    "for gen_id, scores in rouge_data.items():\n",
    "    mask = final_df['generation_id'] == gen_id\n",
    "    if any(mask):\n",
    "        rouge_match_count += 1\n",
    "        for key, value in scores.items():\n",
    "            final_df.loc[mask, key] = value\n",
    "print(f\"Matched ROUGE scores for {rouge_match_count} of {len(rouge_data)} records\")\n",
    "\n",
    "# Add LLM evaluation scores to the DataFrame\n",
    "llm_match_count = 0\n",
    "for post_id, eval_scores in llm_eval_dict.items():\n",
    "    mask = final_df['generation_id'] == post_id\n",
    "    if any(mask):\n",
    "        llm_match_count += 1\n",
    "        for key, value in eval_scores.items():\n",
    "            final_df.loc[mask, key] = value\n",
    "print(f\"Matched LLM evaluations for {llm_match_count} of {len(llm_eval_dict)} records\")\n",
    "\n",
    "# Convert persona columns to boolean if they contain text descriptions\n",
    "for col in persona_columns:\n",
    "    if col in final_df.columns:\n",
    "        # Check if this column contains strings (descriptions) instead of booleans\n",
    "        if final_df[col].dtype == 'object' and isinstance(final_df[col].iloc[0], str):\n",
    "            # If it contains descriptions, set to True (present) for non-empty strings\n",
    "            final_df[col] = final_df[col].notna() & (final_df[col] != '')\n",
    "\n",
    "# Ensure all expected columns are present, even if empty\n",
    "expected_columns = [\n",
    "    'user_id', 'generation_id', 'original_post_id', 'original_text', 'stimulus',\n",
    "    'generation_index', 'generated_text', 'similarity_scores',\n",
    "    'rouge1_fmeasure', 'rouge1_recall', 'rouge1_precision',\n",
    "    'rouge2_fmeasure', 'rouge2_recall', 'rouge2_precision',\n",
    "    'rougel_fmeasure', 'rougel_recall', 'rougel_precision',\n",
    "    'llm_evaluation_authenticity', 'llm_evaluation_style_consistency',\n",
    "    'llm_evaluation_matching_intent'\n",
    "] + persona_columns\n",
    "\n",
    "for col in expected_columns:\n",
    "    if col not in final_df.columns:\n",
    "        final_df[col] = None\n",
    "\n",
    "# Reorder columns to match the reference file\n",
    "final_df = final_df[expected_columns]\n",
    "\n",
    "# Print a summary of the data before saving\n",
    "print(\"\\nData Summary:\")\n",
    "print(f\"Total records: {len(final_df)}\")\n",
    "print(\"Column completion rates:\")\n",
    "for col in expected_columns:\n",
    "    non_null_count = final_df[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null_count}/{len(final_df)} records ({non_null_count/len(final_df)*100:.1f}%)\")\n",
    "\n",
    "# Validate that all required fields have good coverage\n",
    "critical_columns = ['user_id', 'generation_id', 'original_post_id', 'generated_text', \n",
    "                   'similarity_scores', 'rouge1_fmeasure', 'llm_evaluation_authenticity']\n",
    "missing_critical = False\n",
    "for col in critical_columns:\n",
    "    if final_df[col].isna().sum() > len(final_df) * 0.1:  # More than 10% missing\n",
    "        print(f\"WARNING: Critical column '{col}' is missing in more than 10% of records!\")\n",
    "        missing_critical = True\n",
    "\n",
    "# Save the final DataFrame to CSV\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully created combined CSV file at: {output_path}\")\n",
    "print(f\"Total records processed: {len(final_df)}\")\n",
    "\n",
    "if missing_critical:\n",
    "    print(\"\\nWARNING: Some critical columns have significant missing data. Please review the output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averges per Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to /Users/christophhau/Desktop/Research_case/results/dimension_metrics_table_GPT4o.csv\n",
      "\n",
      "Table Preview:\n",
      "            Dimension Rouge1 (Present) Rouge1 (Absent) Rouge1 (Diff)  \\\n",
      "0   stress_indicators            0.209           0.211        -0.002   \n",
      "1   conflict_approach            0.209           0.211        -0.002   \n",
      "2       brevity_style            0.218           0.209         0.009   \n",
      "3  language_formality            0.214           0.210         0.004   \n",
      "4    vocabulary_range            0.208           0.212        -0.003   \n",
      "\n",
      "  Rouge2 (Present) Rouge2 (Absent) Rouge2 (Diff) Rougel (Present)  \\\n",
      "0            0.055           0.056        -0.002            0.149   \n",
      "1            0.056           0.056        -0.001            0.150   \n",
      "2            0.059           0.055         0.004            0.162   \n",
      "3            0.057           0.056         0.001            0.153   \n",
      "4            0.056           0.056        -0.000            0.148   \n",
      "\n",
      "  Rougel (Absent) Rougel (Diff)  ... Similarity Scores (Diff)  \\\n",
      "0           0.151        -0.002  ...                   -0.000   \n",
      "1           0.151        -0.002  ...                   -0.002   \n",
      "2           0.148         0.014  ...                    0.002   \n",
      "3           0.150         0.003  ...                    0.006   \n",
      "4           0.152        -0.004  ...                   -0.002   \n",
      "\n",
      "  Authenticity (Present) Authenticity (Absent) Authenticity (Diff)  \\\n",
      "0                   6.45                  6.57               -0.12   \n",
      "1                   6.55                  6.54                0.01   \n",
      "2                   6.41                  6.58               -0.16   \n",
      "3                   6.54                  6.54                0.00   \n",
      "4                   6.52                  6.55               -0.03   \n",
      "\n",
      "  Style Consistency (Present) Style Consistency (Absent)  \\\n",
      "0                        6.35                       6.50   \n",
      "1                        6.43                       6.48   \n",
      "2                        6.43                       6.48   \n",
      "3                        6.68                       6.41   \n",
      "4                        6.59                       6.43   \n",
      "\n",
      "  Style Consistency (Diff) Matching Intent (Present) Matching Intent (Absent)  \\\n",
      "0                    -0.16                     38.0%                    40.4%   \n",
      "1                    -0.05                     38.5%                    40.3%   \n",
      "2                    -0.05                     37.6%                    40.5%   \n",
      "3                     0.28                     42.4%                    39.1%   \n",
      "4                     0.16                     39.3%                    40.0%   \n",
      "\n",
      "  Matching Intent (Diff)  \n",
      "0                  -2.4%  \n",
      "1                  -1.8%  \n",
      "2                  -2.9%  \n",
      "3                   3.3%  \n",
      "4                  -0.7%  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_json_files(eval_file_path, cleaned_file_path):\n",
    "    \"\"\"\n",
    "    Analyze JSON files and create a table showing metrics by dimensions\n",
    "    \n",
    "    Args:\n",
    "        eval_file_path (str): Path to evaluation results JSON file\n",
    "        cleaned_file_path (str): Path to cleaned output JSON file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Table with metrics by dimensions\n",
    "    \"\"\"\n",
    "    # Load the JSON files\n",
    "    with open(eval_file_path, 'r') as f:\n",
    "        eval_data = json.load(f)\n",
    "    \n",
    "    with open(cleaned_file_path, 'r') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "    \n",
    "    # Extract the individual evaluations and generated posts\n",
    "    evaluations = eval_data['individual_evaluations']\n",
    "    posts = cleaned_data['generated_posts']\n",
    "    \n",
    "    # Create a mapping from generation_id to evaluation data\n",
    "    eval_map = {}\n",
    "    for eval_item in evaluations:\n",
    "        gen_id = eval_item['metadata']['generated_id']\n",
    "        eval_map[gen_id] = eval_item\n",
    "    \n",
    "    # Identify dimensions (persona attributes) - collecting from all posts to ensure we get all dimensions\n",
    "    dimensions = set()\n",
    "    for post in posts:\n",
    "        for key in post.keys():\n",
    "            if key.startswith('persona_'):\n",
    "                dimensions.add(key)\n",
    "    dimensions = list(dimensions)\n",
    "    \n",
    "    # Create a DataFrame with combined data\n",
    "    combined_data = []\n",
    "    \n",
    "    for post in posts:\n",
    "        gen_id = post['generation_id']\n",
    "        if gen_id in eval_map:\n",
    "            eval_item = eval_map[gen_id]\n",
    "            \n",
    "            # Create a row with all necessary data\n",
    "            row = {}\n",
    "            \n",
    "            # Add dimension flags (1 if the dimension exists and is non-empty, 0 otherwise)\n",
    "            for dim in dimensions:\n",
    "                if dim in post and isinstance(post[dim], str):\n",
    "                    row[dim] = 1 if post[dim].strip() != \"\" else 0\n",
    "                else:\n",
    "                    row[dim] = 0\n",
    "            \n",
    "            # Add metrics from evaluation data - with error handling\n",
    "            \n",
    "            # Rouge scores\n",
    "            if 'rouge_scores' in eval_item:\n",
    "                rouge_scores = eval_item['rouge_scores']\n",
    "                for rouge_type, metrics in rouge_scores.items():\n",
    "                    if 'fmeasure' in metrics:\n",
    "                        row[f\"{rouge_type}_fmeasure\"] = metrics['fmeasure']\n",
    "            \n",
    "            # Similarity score\n",
    "            if 'similarity_scores' in eval_item:\n",
    "                row['similarity_scores'] = eval_item['similarity_scores']\n",
    "            \n",
    "            # LLM evaluation scores\n",
    "            if 'llm_evaluation' in eval_item:\n",
    "                llm_eval = eval_item['llm_evaluation']\n",
    "                for key, value in llm_eval.items():\n",
    "                    if isinstance(value, dict) and 'score' in value:\n",
    "                        row[f\"llm_{key}_score\"] = value['score']\n",
    "                    elif key == 'matching_intent':\n",
    "                        row['matching_intent'] = 1 if value else 0\n",
    "            \n",
    "            combined_data.append(row)\n",
    "    \n",
    "    # Create DataFrame from combined data\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    # Calculate metrics by dimension\n",
    "    metrics = [col for col in df.columns if col not in dimensions]\n",
    "    \n",
    "    # Check if we have data to analyze\n",
    "    if df.empty:\n",
    "        print(\"No matching data found between evaluation and generated posts.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        dim_result = {'Dimension': dim.replace('persona_', '')}\n",
    "        \n",
    "        # Filter rows where dimension is present (1) or absent (0)\n",
    "        present = df[df[dim] == 1]\n",
    "        absent = df[df[dim] == 0]\n",
    "        \n",
    "        # Skip if all rows are in one category\n",
    "        if len(present) == 0 or len(absent) == 0:\n",
    "            print(f\"Warning: Dimension {dim} has all values present or all values absent. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        for metric in metrics:\n",
    "            present_avg = present[metric].mean()\n",
    "            absent_avg = absent[metric].mean()\n",
    "            diff = present_avg - absent_avg\n",
    "            \n",
    "            dim_result[f\"{metric}_present\"] = present_avg\n",
    "            dim_result[f\"{metric}_absent\"] = absent_avg\n",
    "            dim_result[f\"{metric}_diff\"] = diff\n",
    "        \n",
    "        results.append(dim_result)\n",
    "    \n",
    "    # Create final results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Format the table for better readability\n",
    "    formatted_table = create_formatted_table(results_df, dimensions, metrics)\n",
    "    \n",
    "    return formatted_table\n",
    "\n",
    "def create_formatted_table(results_df, dimensions, metrics):\n",
    "    \"\"\"\n",
    "    Create a formatted table with metrics as columns and dimensions as rows\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): Results DataFrame\n",
    "        dimensions (list): List of dimension names\n",
    "        metrics (list): List of metric names\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Formatted table\n",
    "    \"\"\"\n",
    "    # Group metrics by type\n",
    "    metric_groups = defaultdict(list)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if 'rouge' in metric:\n",
    "            base_name = metric.split('_')[0]\n",
    "            metric_groups[base_name].append(metric)\n",
    "        elif 'llm_' in metric:\n",
    "            base_name = metric.replace('llm_', '').replace('_score', '')\n",
    "            metric_groups[base_name].append(metric)\n",
    "        else:\n",
    "            metric_groups[metric].append(metric)\n",
    "    \n",
    "    # Create the table with clean column names\n",
    "    table_data = []\n",
    "    \n",
    "    for index, row in results_df.iterrows():\n",
    "        table_row = {'Dimension': row['Dimension']}\n",
    "        \n",
    "        for group_name, group_metrics in metric_groups.items():\n",
    "            for metric in group_metrics:\n",
    "                if f\"{metric}_present\" in row:\n",
    "                    present_val = row[f\"{metric}_present\"]\n",
    "                    absent_val = row[f\"{metric}_absent\"]\n",
    "                    diff_val = row[f\"{metric}_diff\"]\n",
    "                    \n",
    "                    metric_display = group_name.replace('_', ' ').title()\n",
    "                    \n",
    "                    # Format values based on metric type\n",
    "                    if metric == 'matching_intent':\n",
    "                        present_str = f\"{present_val:.1%}\"\n",
    "                        absent_str = f\"{absent_val:.1%}\"\n",
    "                        diff_str = f\"{diff_val:.1%}\"\n",
    "                    elif 'rouge' in metric or 'similarity' in metric:\n",
    "                        present_str = f\"{present_val:.3f}\"\n",
    "                        absent_str = f\"{absent_val:.3f}\"\n",
    "                        diff_str = f\"{diff_val:.3f}\"\n",
    "                    else:  # llm scores\n",
    "                        present_str = f\"{present_val:.2f}\"\n",
    "                        absent_str = f\"{absent_val:.2f}\"\n",
    "                        diff_str = f\"{diff_val:.2f}\"\n",
    "                    \n",
    "                    table_row[f\"{metric_display} (Present)\"] = present_str\n",
    "                    table_row[f\"{metric_display} (Absent)\"] = absent_str\n",
    "                    table_row[f\"{metric_display} (Diff)\"] = diff_str\n",
    "        \n",
    "        table_data.append(table_row)\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis\"\"\"\n",
    "    eval_file = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/merged_evaluations.json\"\n",
    "    cleaned_file = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/generated_posts.json\"\n",
    "    \n",
    "    result_table = analyze_json_files(eval_file, cleaned_file)\n",
    "    \n",
    "    # Save the result to CSV\n",
    "    output_path = \"/Users/christophhau/Desktop/Research_case/results/dimension_metrics_table_GPT4o.csv\"\n",
    "    result_table.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved to {output_path}\")\n",
    "    print(\"\\nTable Preview:\")\n",
    "    print(result_table.head())\n",
    "    \n",
    "    # Also save a more compact version with just the differences\n",
    "    diff_cols = ['Dimension'] + [col for col in result_table.columns if '(Diff)' in col]\n",
    "    diff_table = result_table[diff_cols]\n",
    "    diff_table.to_csv(output_path.replace('.csv', '_diff_only.csv'), index=False)\n",
    "    \n",
    "    return result_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama 70b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to /Users/christophhau/Desktop/Research_case/results/dimension_metrics_table_Llama70bft.csv\n",
      "\n",
      "Table Preview:\n",
      "            Dimension Rouge1 (Present) Rouge1 (Absent) Rouge1 (Diff)  \\\n",
      "0   stress_indicators            0.315           0.314         0.001   \n",
      "1   conflict_approach            0.298           0.319        -0.021   \n",
      "2       brevity_style            0.316           0.314         0.002   \n",
      "3  language_formality            0.315           0.314         0.001   \n",
      "4    vocabulary_range            0.344           0.307         0.038   \n",
      "\n",
      "  Rouge2 (Present) Rouge2 (Absent) Rouge2 (Diff) Rougel (Present)  \\\n",
      "0            0.119           0.121        -0.002            0.244   \n",
      "1            0.110           0.124        -0.014            0.231   \n",
      "2            0.111           0.124        -0.012            0.239   \n",
      "3            0.120           0.121        -0.001            0.245   \n",
      "4            0.147           0.114         0.033            0.267   \n",
      "\n",
      "  Rougel (Absent) Rougel (Diff)  ... Similarity Scores (Diff)  \\\n",
      "0           0.244         0.001  ...                    0.002   \n",
      "1           0.247        -0.016  ...                   -0.003   \n",
      "2           0.245        -0.006  ...                    0.005   \n",
      "3           0.243         0.002  ...                   -0.002   \n",
      "4           0.238         0.029  ...                    0.002   \n",
      "\n",
      "  Authenticity (Present) Authenticity (Absent) Authenticity (Diff)  \\\n",
      "0                   5.91                  6.07               -0.15   \n",
      "1                   6.05                  6.03                0.02   \n",
      "2                   6.20                  5.97                0.23   \n",
      "3                   5.92                  6.07               -0.15   \n",
      "4                   6.09                  6.02                0.07   \n",
      "\n",
      "  Style Consistency (Present) Style Consistency (Absent)  \\\n",
      "0                        6.80                       6.85   \n",
      "1                        6.90                       6.82   \n",
      "2                        7.03                       6.77   \n",
      "3                        6.74                       6.87   \n",
      "4                        6.94                       6.81   \n",
      "\n",
      "  Style Consistency (Diff) Matching Intent (Present) Matching Intent (Absent)  \\\n",
      "0                    -0.05                     78.0%                    78.9%   \n",
      "1                     0.08                     78.4%                    78.8%   \n",
      "2                     0.26                     82.9%                    77.2%   \n",
      "3                    -0.13                     76.3%                    79.6%   \n",
      "4                     0.13                     79.1%                    78.6%   \n",
      "\n",
      "  Matching Intent (Diff)  \n",
      "0                  -0.9%  \n",
      "1                  -0.4%  \n",
      "2                   5.7%  \n",
      "3                  -3.2%  \n",
      "4                   0.5%  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_json_files(eval_file_path, cleaned_file_path):\n",
    "    \"\"\"\n",
    "    Analyze JSON files and create a table showing metrics by dimensions\n",
    "    \n",
    "    Args:\n",
    "        eval_file_path (str): Path to evaluation results JSON file\n",
    "        cleaned_file_path (str): Path to cleaned output JSON file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Table with metrics by dimensions\n",
    "    \"\"\"\n",
    "    # Load the JSON files\n",
    "    with open(eval_file_path, 'r') as f:\n",
    "        eval_data = json.load(f)\n",
    "    \n",
    "    with open(cleaned_file_path, 'r') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "    \n",
    "    # Extract the individual evaluations and generated posts\n",
    "    evaluations = eval_data['individual_evaluations']\n",
    "    posts = cleaned_data['generated_posts']\n",
    "    \n",
    "    # Create a mapping from generation_id to evaluation data\n",
    "    eval_map = {}\n",
    "    for eval_item in evaluations:\n",
    "        gen_id = eval_item['metadata']['generated_id']\n",
    "        eval_map[gen_id] = eval_item\n",
    "    \n",
    "    # Identify dimensions (persona attributes) - collecting from all posts to ensure we get all dimensions\n",
    "    dimensions = set()\n",
    "    for post in posts:\n",
    "        for key in post.keys():\n",
    "            if key.startswith('persona_'):\n",
    "                dimensions.add(key)\n",
    "    dimensions = list(dimensions)\n",
    "    \n",
    "    # Create a DataFrame with combined data\n",
    "    combined_data = []\n",
    "    \n",
    "    for post in posts:\n",
    "        gen_id = post['generation_id']\n",
    "        if gen_id in eval_map:\n",
    "            eval_item = eval_map[gen_id]\n",
    "            \n",
    "            # Create a row with all necessary data\n",
    "            row = {}\n",
    "            \n",
    "            # Add dimension flags (1 if the dimension exists and is non-empty, 0 otherwise)\n",
    "            for dim in dimensions:\n",
    "                if dim in post and isinstance(post[dim], str):\n",
    "                    row[dim] = 1 if post[dim].strip() != \"\" else 0\n",
    "                else:\n",
    "                    row[dim] = 0\n",
    "            \n",
    "            # Add metrics from evaluation data - with error handling\n",
    "            \n",
    "            # Rouge scores\n",
    "            if 'rouge_scores' in eval_item:\n",
    "                rouge_scores = eval_item['rouge_scores']\n",
    "                for rouge_type, metrics in rouge_scores.items():\n",
    "                    if 'fmeasure' in metrics:\n",
    "                        row[f\"{rouge_type}_fmeasure\"] = metrics['fmeasure']\n",
    "            \n",
    "            # Similarity score\n",
    "            if 'similarity_scores' in eval_item:\n",
    "                row['similarity_scores'] = eval_item['similarity_scores']\n",
    "            \n",
    "            # LLM evaluation scores\n",
    "            if 'llm_evaluation' in eval_item:\n",
    "                llm_eval = eval_item['llm_evaluation']\n",
    "                for key, value in llm_eval.items():\n",
    "                    if isinstance(value, dict) and 'score' in value:\n",
    "                        row[f\"llm_{key}_score\"] = value['score']\n",
    "                    elif key == 'matching_intent':\n",
    "                        row['matching_intent'] = 1 if value else 0\n",
    "            \n",
    "            combined_data.append(row)\n",
    "    \n",
    "    # Create DataFrame from combined data\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    # Calculate metrics by dimension\n",
    "    metrics = [col for col in df.columns if col not in dimensions]\n",
    "    \n",
    "    # Check if we have data to analyze\n",
    "    if df.empty:\n",
    "        print(\"No matching data found between evaluation and generated posts.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        dim_result = {'Dimension': dim.replace('persona_', '')}\n",
    "        \n",
    "        # Filter rows where dimension is present (1) or absent (0)\n",
    "        present = df[df[dim] == 1]\n",
    "        absent = df[df[dim] == 0]\n",
    "        \n",
    "        # Skip if all rows are in one category\n",
    "        if len(present) == 0 or len(absent) == 0:\n",
    "            print(f\"Warning: Dimension {dim} has all values present or all values absent. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        for metric in metrics:\n",
    "            present_avg = present[metric].mean()\n",
    "            absent_avg = absent[metric].mean()\n",
    "            diff = present_avg - absent_avg\n",
    "            \n",
    "            dim_result[f\"{metric}_present\"] = present_avg\n",
    "            dim_result[f\"{metric}_absent\"] = absent_avg\n",
    "            dim_result[f\"{metric}_diff\"] = diff\n",
    "        \n",
    "        results.append(dim_result)\n",
    "    \n",
    "    # Create final results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Format the table for better readability\n",
    "    formatted_table = create_formatted_table(results_df, dimensions, metrics)\n",
    "    \n",
    "    return formatted_table\n",
    "\n",
    "def create_formatted_table(results_df, dimensions, metrics):\n",
    "    \"\"\"\n",
    "    Create a formatted table with metrics as columns and dimensions as rows\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): Results DataFrame\n",
    "        dimensions (list): List of dimension names\n",
    "        metrics (list): List of metric names\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Formatted table\n",
    "    \"\"\"\n",
    "    # Group metrics by type\n",
    "    metric_groups = defaultdict(list)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if 'rouge' in metric:\n",
    "            base_name = metric.split('_')[0]\n",
    "            metric_groups[base_name].append(metric)\n",
    "        elif 'llm_' in metric:\n",
    "            base_name = metric.replace('llm_', '').replace('_score', '')\n",
    "            metric_groups[base_name].append(metric)\n",
    "        else:\n",
    "            metric_groups[metric].append(metric)\n",
    "    \n",
    "    # Create the table with clean column names\n",
    "    table_data = []\n",
    "    \n",
    "    for index, row in results_df.iterrows():\n",
    "        table_row = {'Dimension': row['Dimension']}\n",
    "        \n",
    "        for group_name, group_metrics in metric_groups.items():\n",
    "            for metric in group_metrics:\n",
    "                if f\"{metric}_present\" in row:\n",
    "                    present_val = row[f\"{metric}_present\"]\n",
    "                    absent_val = row[f\"{metric}_absent\"]\n",
    "                    diff_val = row[f\"{metric}_diff\"]\n",
    "                    \n",
    "                    metric_display = group_name.replace('_', ' ').title()\n",
    "                    \n",
    "                    # Format values based on metric type\n",
    "                    if metric == 'matching_intent':\n",
    "                        present_str = f\"{present_val:.1%}\"\n",
    "                        absent_str = f\"{absent_val:.1%}\"\n",
    "                        diff_str = f\"{diff_val:.1%}\"\n",
    "                    elif 'rouge' in metric or 'similarity' in metric:\n",
    "                        present_str = f\"{present_val:.3f}\"\n",
    "                        absent_str = f\"{absent_val:.3f}\"\n",
    "                        diff_str = f\"{diff_val:.3f}\"\n",
    "                    else:  # llm scores\n",
    "                        present_str = f\"{present_val:.2f}\"\n",
    "                        absent_str = f\"{absent_val:.2f}\"\n",
    "                        diff_str = f\"{diff_val:.2f}\"\n",
    "                    \n",
    "                    table_row[f\"{metric_display} (Present)\"] = present_str\n",
    "                    table_row[f\"{metric_display} (Absent)\"] = absent_str\n",
    "                    table_row[f\"{metric_display} (Diff)\"] = diff_str\n",
    "        \n",
    "        table_data.append(table_row)\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis\"\"\"\n",
    "    eval_file = \"/Users/christophhau/Desktop/Research_case/results/fine_tuned_llama/eval_results.json\"\n",
    "    cleaned_file = \"/Users/christophhau/Desktop/Research_case/results/fine_tuned_llama/cleaned_output.json\"\n",
    "    \n",
    "    result_table = analyze_json_files(eval_file, cleaned_file)\n",
    "    \n",
    "    # Save the result to CSV\n",
    "    output_path = \"/Users/christophhau/Desktop/Research_case/results/dimension_metrics_table_Llama70bft.csv\"\n",
    "    result_table.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved to {output_path}\")\n",
    "    print(\"\\nTable Preview:\")\n",
    "    print(result_table.head())\n",
    "    \n",
    "    # Also save a more compact version with just the differences\n",
    "    diff_cols = ['Dimension'] + [col for col in result_table.columns if '(Diff)' in col]\n",
    "    diff_table = result_table[diff_cols]\n",
    "    diff_table.to_csv(output_path.replace('.csv', '_diff_only.csv'), index=False)\n",
    "    \n",
    "    return result_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA-formatted LaTeX table saved to /Users/christophhau/Desktop/Research_case/results/GPT4o_apa_table.tex\n",
      "Compact APA-formatted LaTeX table saved to /Users/christophhau/Desktop/Research_case/results/GPT4o_apa_table_compact.tex\n",
      "APA-formatted LaTeX table saved to /Users/christophhau/Desktop/Research_case/results/apa_tables/table_rouge_scores.tex\n",
      "APA-formatted LaTeX table saved to /Users/christophhau/Desktop/Research_case/results/apa_tables/table_similarity.tex\n",
      "APA-formatted LaTeX table saved to /Users/christophhau/Desktop/Research_case/results/apa_tables/table_authenticity.tex\n",
      "APA-formatted LaTeX table saved to /Users/christophhau/Desktop/Research_case/results/apa_tables/table_style.tex\n",
      "Compact APA-formatted LaTeX table saved to /Users/christophhau/Desktop/Research_case/results/apa_tables/table_intent.tex\n",
      "Created 5 APA-formatted tables in /Users/christophhau/Desktop/Research_case/results/apa_tables\n",
      "Conversion complete. APA tables created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def convert_csv_to_apa_latex(csv_path, output_path, table_number=1, table_title=\"Comparison of Metrics Across Different Dimensions\", table_note=\"Note. All metrics are presented with mean values. 'Present' indicates the dimension was included in the text, 'Absent' indicates it was not.\"):\n",
    "    \"\"\"\n",
    "    Convert a CSV file with dimension metrics to an APA-formatted LaTeX table\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file\n",
    "        output_path (str): Path where to save the LaTeX file\n",
    "        table_number (int): Table number for the APA table\n",
    "        table_title (str): Title for the APA table\n",
    "        table_note (str): Note to appear at the bottom of the table\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Clean up column names\n",
    "    clean_cols = []\n",
    "    for col in df.columns:\n",
    "        # Extract metric name and type (Present/Absent/Diff)\n",
    "        if \"(\" in col and \")\" in col:\n",
    "            metric, measure_type = col.split(\"(\")\n",
    "            measure_type = measure_type.replace(\")\", \"\").strip()\n",
    "            clean_cols.append((metric.strip(), measure_type))\n",
    "        else:\n",
    "            clean_cols.append((col, \"\"))\n",
    "    \n",
    "    # Group metrics\n",
    "    metrics = {}\n",
    "    for col_name, measure_type in clean_cols:\n",
    "        if col_name == \"Dimension\":\n",
    "            continue\n",
    "        if col_name not in metrics:\n",
    "            metrics[col_name] = []\n",
    "        metrics[col_name].append(measure_type)\n",
    "    \n",
    "    # Create LaTeX header\n",
    "    latex = []\n",
    "    latex.append(\"\\\\begin{table}\")\n",
    "    latex.append(\"\\\\caption{\" + table_title + \"}\")\n",
    "    latex.append(\"\\\\label{table\" + str(table_number) + \"}\")\n",
    "    latex.append(\"\\\\begin{tabular}{l\" + \"ccc\" * len(metrics) + \"}\")\n",
    "    latex.append(\"\\\\hline\")\n",
    "    \n",
    "    # Create column headers\n",
    "    header1 = [\"\\\\multirow{2}{*}{Dimension}\"]\n",
    "    header2 = [\"\"]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        header1.append(\"\\\\multicolumn{3}{c}{\" + metric + \"}\")\n",
    "        header2.extend([\"Present\", \"Absent\", \"Diff\"])\n",
    "    \n",
    "    latex.append(\" & \".join(header1) + \" \\\\\\\\\")\n",
    "    latex.append(\" & \".join(header2) + \" \\\\\\\\\")\n",
    "    latex.append(\"\\\\hline\")\n",
    "    \n",
    "    # Create table rows\n",
    "    for _, row in df.iterrows():\n",
    "        dimension = row[\"Dimension\"].replace(\"_\", \" \").title()\n",
    "        row_data = [dimension]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            for measure in [\"Present\", \"Absent\", \"Diff\"]:\n",
    "                col_name = f\"{metric} ({measure})\"\n",
    "                if col_name in df.columns:\n",
    "                    # Get the value and format it\n",
    "                    value = row[col_name]\n",
    "                    \n",
    "                    # Format based on the value type (percentage, float, etc.)\n",
    "                    if isinstance(value, str):\n",
    "                        if \"%\" in value:\n",
    "                            # It's a percentage\n",
    "                            value_float = float(value.replace(\"%\", \"\"))\n",
    "                            formatted_value = f\"{value_float:.1f}\\\\%\"\n",
    "                        else:\n",
    "                            # Try to convert to float\n",
    "                            try:\n",
    "                                value_float = float(value)\n",
    "                                if value_float >= 0.01:\n",
    "                                    formatted_value = f\"{value_float:.2f}\"\n",
    "                                else:\n",
    "                                    formatted_value = f\"{value_float:.3f}\"\n",
    "                            except ValueError:\n",
    "                                formatted_value = value\n",
    "                    else:\n",
    "                        # Numeric value\n",
    "                        if value >= 0.01:\n",
    "                            formatted_value = f\"{value:.2f}\"\n",
    "                        else:\n",
    "                            formatted_value = f\"{value:.3f}\"\n",
    "                            \n",
    "                    row_data.append(formatted_value)\n",
    "                else:\n",
    "                    row_data.append(\"\")\n",
    "        \n",
    "        latex.append(\" & \".join(row_data) + \" \\\\\\\\\")\n",
    "    \n",
    "    # Close the table\n",
    "    latex.append(\"\\\\hline\")\n",
    "    latex.append(\"\\\\end{tabular}\")\n",
    "    \n",
    "    # Add table note\n",
    "    if table_note:\n",
    "        latex.append(\"\\\\begin{tablenotes}\")\n",
    "        latex.append(\"\\\\small\")\n",
    "        latex.append(\"\\\\item \" + table_note)\n",
    "        latex.append(\"\\\\end{tablenotes}\")\n",
    "    \n",
    "    latex.append(\"\\\\end{table}\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(latex))\n",
    "    \n",
    "    print(f\"APA-formatted LaTeX table saved to {output_path}\")\n",
    "    \n",
    "    # Also return the LaTeX code as a string\n",
    "    return \"\\n\".join(latex)\n",
    "\n",
    "def convert_csv_to_apa_compact(csv_path, output_path, table_number=1, table_title=\"Impact of Textual Dimensions on Generation Quality\", show_only_diff=True):\n",
    "    \"\"\"\n",
    "    Convert a CSV file with dimension metrics to a more compact APA-formatted LaTeX table\n",
    "    that only shows differences or a more focused set of metrics\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file\n",
    "        output_path (str): Path where to save the LaTeX file\n",
    "        table_number (int): Table number for the APA table\n",
    "        table_title (str): Title for the APA table\n",
    "        show_only_diff (bool): If True, only show the difference columns\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Filter to only include Dimension and Diff columns if requested\n",
    "    if show_only_diff:\n",
    "        diff_cols = ['Dimension'] + [col for col in df.columns if '(Diff)' in col]\n",
    "        df = df[diff_cols]\n",
    "    \n",
    "    # Clean up column names for the diff-only columns\n",
    "    if show_only_diff:\n",
    "        clean_cols = {}\n",
    "        for col in df.columns:\n",
    "            if col == 'Dimension':\n",
    "                clean_cols[col] = col\n",
    "            else:\n",
    "                # Extract just the metric name without \"(Diff)\"\n",
    "                metric = col.replace(\" (Diff)\", \"\")\n",
    "                clean_cols[col] = metric\n",
    "        \n",
    "        df = df.rename(columns=clean_cols)\n",
    "    \n",
    "    # Format the values for better readability\n",
    "    for col in df.columns:\n",
    "        if col == 'Dimension':\n",
    "            continue\n",
    "            \n",
    "        # Format numeric values\n",
    "        if df[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
    "            df[col] = df[col].map(lambda x: f\"{x:.2f}\" if abs(x) >= 0.01 else f\"{x:.3f}\")\n",
    "        elif df[col].dtype == object:  # String values\n",
    "            # Try to convert strings to floats for formatting\n",
    "            df[col] = df[col].map(lambda x: \n",
    "                f\"{float(x.replace('%', '')):.1f}\\\\%\" if isinstance(x, str) and '%' in x\n",
    "                else (f\"{float(x):.2f}\" if isinstance(x, str) and abs(float(x)) >= 0.01 \n",
    "                      else f\"{float(x):.3f}\" if isinstance(x, str)\n",
    "                      else x))\n",
    "    \n",
    "    # Create LaTeX header\n",
    "    latex = []\n",
    "    latex.append(\"\\\\begin{table}\")\n",
    "    latex.append(\"\\\\caption{\" + table_title + \"}\")\n",
    "    latex.append(\"\\\\label{table\" + str(table_number) + \"}\")\n",
    "    \n",
    "    # Calculate column formatting based on number of metrics\n",
    "    num_metrics = len(df.columns) - 1  # Subtract 1 for the Dimension column\n",
    "    column_format = \"l\" + \"c\" * num_metrics\n",
    "    \n",
    "    latex.append(\"\\\\begin{tabular}{\" + column_format + \"}\")\n",
    "    latex.append(\"\\\\hline\")\n",
    "    \n",
    "    # Create column headers - simple version for diff-only table\n",
    "    header = [col.replace(\"_\", \" \").title() for col in df.columns]\n",
    "    latex.append(\" & \".join(header) + \" \\\\\\\\\")\n",
    "    latex.append(\"\\\\hline\")\n",
    "    \n",
    "    # Create table rows\n",
    "    for _, row in df.iterrows():\n",
    "        # Format the dimension name nicely\n",
    "        dimension = row[\"Dimension\"].replace(\"_\", \" \").title()\n",
    "        row_data = [dimension]\n",
    "        \n",
    "        # Add other columns\n",
    "        for col in df.columns[1:]:  # Skip the Dimension column\n",
    "            row_data.append(str(row[col]))\n",
    "        \n",
    "        latex.append(\" & \".join(row_data) + \" \\\\\\\\\")\n",
    "    \n",
    "    # Close the table\n",
    "    latex.append(\"\\\\hline\")\n",
    "    latex.append(\"\\\\end{tabular}\")\n",
    "    \n",
    "    # Add a note about what the values represent\n",
    "    if show_only_diff:\n",
    "        note = \"Note. Values represent the difference in metrics when the dimension is present versus absent. Positive values indicate higher performance when the dimension is present.\"\n",
    "        latex.append(\"\\\\begin{tablenotes}\")\n",
    "        latex.append(\"\\\\small\")\n",
    "        latex.append(\"\\\\item \" + note)\n",
    "        latex.append(\"\\\\end{tablenotes}\")\n",
    "    \n",
    "    latex.append(\"\\\\end{table}\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(latex))\n",
    "    \n",
    "    print(f\"Compact APA-formatted LaTeX table saved to {output_path}\")\n",
    "    \n",
    "    # Also return the LaTeX code as a string\n",
    "    return \"\\n\".join(latex)\n",
    "\n",
    "def create_multiple_tables_by_metric_group(csv_path, output_dir, metric_groups=None):\n",
    "    \"\"\"\n",
    "    Create multiple APA tables based on grouped metrics\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file\n",
    "        output_dir (str): Directory to save the LaTeX files\n",
    "        metric_groups (dict): Dictionary mapping group names to lists of metric name patterns\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Default metric groups if none provided\n",
    "    if metric_groups is None:\n",
    "        metric_groups = {\n",
    "            \"rouge\": [\"Rouge\"],\n",
    "            \"similarity\": [\"Similarity\"],\n",
    "            \"llm_scores\": [\"Authenticity\", \"Style\", \"Consistency\"],\n",
    "            \"intent\": [\"Matching\", \"Intent\"]\n",
    "        }\n",
    "    \n",
    "    # Process each metric group\n",
    "    for group_idx, (group_name, patterns) in enumerate(metric_groups.items(), 1):\n",
    "        # Filter columns that match the current group patterns\n",
    "        group_cols = ['Dimension']\n",
    "        for col in df.columns:\n",
    "            if col == 'Dimension':\n",
    "                continue\n",
    "            \n",
    "            # Check if any pattern matches this column\n",
    "            if any(pattern.lower() in col.lower() for pattern in patterns):\n",
    "                group_cols.append(col)\n",
    "        \n",
    "        # Skip if no columns match (other than Dimension)\n",
    "        if len(group_cols) <= 1:\n",
    "            print(f\"No matching columns for group {group_name}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Create a filtered DataFrame with only the relevant columns\n",
    "        group_df = df[group_cols]\n",
    "        \n",
    "        # Save to CSV temporarily\n",
    "        temp_csv = os.path.join(output_dir, f\"temp_{group_name}.csv\")\n",
    "        group_df.to_csv(temp_csv, index=False)\n",
    "        \n",
    "        # Create and save the LaTeX table\n",
    "        output_file = os.path.join(output_dir, f\"table_{group_name}.tex\")\n",
    "        title = f\"Impact of Dimensions on {group_name.replace('_', ' ').title()} Metrics\"\n",
    "        \n",
    "        # Call the appropriate function based on the group\n",
    "        if \"intent\" in group_name.lower():\n",
    "            # For binary metrics like matching_intent\n",
    "            convert_csv_to_apa_compact(temp_csv, output_file, table_number=group_idx, table_title=title)\n",
    "        else:\n",
    "            # For continuous metrics\n",
    "            convert_csv_to_apa_latex(temp_csv, output_file, table_number=group_idx, table_title=title)\n",
    "        \n",
    "        # Remove temporary CSV\n",
    "        os.remove(temp_csv)\n",
    "    \n",
    "    print(f\"Created {len(metric_groups)} APA-formatted tables in {output_dir}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to convert CSV to APA tables\"\"\"\n",
    "    input_csv = \"/Users/christophhau/Desktop/Research_case/results/dimension_metrics_table_GPT4o.csv\"\n",
    "    diff_only_csv = \"/Users/christophhau/Desktop/Research_case/results/dimension_metrics_table_GPT4o_diff_only.csv\"\n",
    "    output_tex = \"/Users/christophhau/Desktop/Research_case/results/GPT4o_apa_table.tex\"\n",
    "    output_compact_tex = \"/Users/christophhau/Desktop/Research_case/results/GPT4o_apa_table_compact.tex\"\n",
    "    output_dir = \"/Users/christophhau/Desktop/Research_case/results/apa_tables\"\n",
    "    \n",
    "    # Create a full APA table\n",
    "    convert_csv_to_apa_latex(input_csv, output_tex)\n",
    "    \n",
    "    # Create a compact version with only differences\n",
    "    convert_csv_to_apa_compact(diff_only_csv, output_compact_tex)\n",
    "    \n",
    "    # Create multiple tables based on metric groups\n",
    "    metric_groups = {\n",
    "        \"rouge_scores\": [\"Rouge1\", \"Rouge2\", \"RougeL\"],\n",
    "        \"similarity\": [\"Similarity\"],\n",
    "        \"authenticity\": [\"Authenticity\"],\n",
    "        \"style\": [\"Style\"],\n",
    "        \"intent\": [\"Matching Intent\"]\n",
    "    }\n",
    "    create_multiple_tables_by_metric_group(input_csv, output_dir, metric_groups)\n",
    "    \n",
    "    print(\"Conversion complete. APA tables created successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade technische Daten aus: /Users/christophhau/Desktop/Research_case/results/fine_tune03/eval_results.json\n",
      "Lade qualitative Daten aus: /Users/christophhau/Desktop/Research_case/results/fine_tune03/judge/evaluation_results_20250325_121256.json\n",
      "Erstelle Mapping für qualitative Bewertungen...\n",
      "Führe Daten zusammen...\n",
      "Zusammenführung abgeschlossen:\n",
      "- Gefundene technische Bewertungen: 8000\n",
      "- Gefundene qualitative Bewertungen: 8000\n",
      "- Erfolgreiche Matches: 8000\n",
      "- Einträge ohne Match: 0\n",
      "Zusammengeführte Daten wurden in /Users/christophhau/Desktop/Research_case/results/fine_tune03/merged_evaluations.json gespeichert.\n",
      "Beispieleinträge aus den zusammengeführten Daten:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_id</th>\n",
       "      <th>rouge1_f1</th>\n",
       "      <th>rouge2_f1</th>\n",
       "      <th>rougeL_f1</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>authenticity_score</th>\n",
       "      <th>style_consistency_score</th>\n",
       "      <th>matching_intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.016827313022034e+18_gen_0</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.016827313022034e+18_gen_1</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.826995</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>809529043.0_gen_0</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.751750</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>809529043.0_gen_1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.733960</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1530958506.0_gen_0</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.893119</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  generated_id  rouge1_f1  rouge2_f1  rougeL_f1  \\\n",
       "0  1.016827313022034e+18_gen_0   0.336842   0.107527   0.252632   \n",
       "1  1.016827313022034e+18_gen_1   0.293333   0.082192   0.266667   \n",
       "2            809529043.0_gen_0   0.129870   0.026667   0.129870   \n",
       "3            809529043.0_gen_1   0.100000   0.000000   0.075000   \n",
       "4           1530958506.0_gen_0   0.530612   0.291667   0.510204   \n",
       "\n",
       "   similarity_score  authenticity_score  style_consistency_score  \\\n",
       "0          0.809191                   7                        6   \n",
       "1          0.826995                   6                        5   \n",
       "2          0.751750                   8                        9   \n",
       "3          0.733960                   7                        8   \n",
       "4          0.893119                   7                        6   \n",
       "\n",
       "   matching_intent  \n",
       "0            False  \n",
       "1            False  \n",
       "2             True  \n",
       "3            False  \n",
       "4            False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistiken zu den Metriken:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1_f1</th>\n",
       "      <th>rouge2_f1</th>\n",
       "      <th>rougeL_f1</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>authenticity_score</th>\n",
       "      <th>style_consistency_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.210983</td>\n",
       "      <td>0.056085</td>\n",
       "      <td>0.150832</td>\n",
       "      <td>0.766283</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.106966</td>\n",
       "      <td>0.072360</td>\n",
       "      <td>0.086273</td>\n",
       "      <td>0.068518</td>\n",
       "      <td>1.387224</td>\n",
       "      <td>1.756182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.724762</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.135758</td>\n",
       "      <td>0.771792</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.188754</td>\n",
       "      <td>0.814586</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.954009</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rouge1_f1    rouge2_f1    rougeL_f1  similarity_score  \\\n",
       "count  8000.000000  8000.000000  8000.000000       8000.000000   \n",
       "mean      0.210983     0.056085     0.150832          0.766283   \n",
       "std       0.106966     0.072360     0.086273          0.068518   \n",
       "min       0.000000     0.000000     0.000000          0.458403   \n",
       "25%       0.135593     0.000000     0.095238          0.724762   \n",
       "50%       0.200000     0.035088     0.135758          0.771792   \n",
       "75%       0.275000     0.079208     0.188754          0.814586   \n",
       "max       0.760000     0.750000     0.760000          0.954009   \n",
       "\n",
       "       authenticity_score  style_consistency_score  \n",
       "count         8000.000000              8000.000000  \n",
       "mean             6.540000                 6.469000  \n",
       "std              1.387224                 1.756182  \n",
       "min              0.000000                 0.000000  \n",
       "25%              6.000000                 5.000000  \n",
       "50%              7.000000                 7.000000  \n",
       "75%              8.000000                 8.000000  \n",
       "max              9.000000                10.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intent-Übereinstimmung:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matching_intent\n",
       "False    4812\n",
       "True     3188\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rouge_scores\": {\n",
      "    \"rouge1\": {\n",
      "      \"precision\": 0.3076923076923077,\n",
      "      \"recall\": 0.37209302325581395,\n",
      "      \"fmeasure\": 0.3368421052631579\n",
      "    },\n",
      "    \"rouge2\": {\n",
      "      \"precision\": 0.09803921568627451,\n",
      "      \"recall\": 0.11904761904761904,\n",
      "      \"fmeasure\": 0.1075268817204301\n",
      "    },\n",
      "    \"rougeL\": {\n",
      "      \"precision\": 0.23076923076923078,\n",
      "      \"recall\": 0.27906976744186046,\n",
      "      \"fmeasure\": 0.25263157894736843\n",
      "    }\n",
      "  },\n",
      "  \"similarity_scores\": 0.8091909885406494,\n",
      "  \"metadata\": {\n",
      "    \"original_id\": 1.69015609e+18,\n",
      "    \"generated_id\": \"1.016827313022034e+18_gen_0\",\n",
      "    \"timestamp\": \"2025-01-29T14:49:41.221139+00:00\"\n",
      "  },\n",
      "  \"llm_evaluation\": {\n",
      "    \"authenticity\": {\n",
      "      \"score\": 7,\n",
      "      \"explanation\": \"The generated post captures the user's skepticism towards government actions and highlights the issue of data privacy, aligning with their libertarian viewpoint. However, it introduces a more conversational and rhetorical style with phrases like 'Uncle Sam' and 'Why am I not surprised?' which slightly deviates from the original's more straightforward tone.\"\n",
      "    },\n",
      "    \"style_consistency\": {\n",
      "      \"score\": 6,\n",
      "      \"explanation\": \"While the generated post maintains an assertive and critical tone, it incorporates a more casual and rhetorical style, using emojis and hashtags, which is less consistent with the original's direct and formal approach. The original post is more concise and focused, whereas the generated post adds additional commentary and rhetorical questions.\"\n",
      "    },\n",
      "    \"matching_intent\": false,\n",
      "    \"overall_feedback\": \"The generated post effectively captures the user's skepticism and advocacy for privacy rights, aligning with their libertarian beliefs. However, it diverges from the original intent by introducing a broader commentary on power and control, and a call for transparency, which were not explicitly present in the original post. The style is more conversational and less formal, which affects both authenticity and style consistency.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # JSON Merger für Evaluationsdaten\n",
    "# \n",
    "# Dieses Notebook führt zwei JSON-Dateien zusammen:\n",
    "# 1. Eine Datei mit technischen Metriken (ROUGE-Scores, Similarity Scores)\n",
    "# 2. Eine Datei mit qualitativen LLM-Evaluationen\n",
    "#\n",
    "# Das Ergebnis ist eine zusammengeführte JSON-Datei, die beide Arten von Evaluationen enthält.\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Funktionen zum Laden und Zusammenführen der Dateien\n",
    "\n",
    "# %%\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Lädt eine JSON-Datei und gibt deren Inhalt zurück.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der Datei {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# %%\n",
    "def merge_json_files(technical_file, qualitative_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Führt die technische und qualitative JSON-Dateien zusammen.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    technical_file : str\n",
    "        Pfad zur technischen JSON-Datei (individual_evaluations)\n",
    "    qualitative_file : str\n",
    "        Pfad zur qualitativen JSON-Datei (results)\n",
    "    output_file : str, optional\n",
    "        Ausgabedatei für die zusammengeführten Daten.\n",
    "        Wenn None, wird keine Datei gespeichert, sondern nur das Ergebnis zurückgegeben.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Die zusammengeführten Daten\n",
    "    \"\"\"\n",
    "    # Dateien laden\n",
    "    print(f\"Lade technische Daten aus: {technical_file}\")\n",
    "    technical_data = load_json_file(technical_file)\n",
    "    \n",
    "    print(f\"Lade qualitative Daten aus: {qualitative_file}\")\n",
    "    qualitative_data = load_json_file(qualitative_file)\n",
    "    \n",
    "    if not technical_data or not qualitative_data:\n",
    "        print(\"Fehler beim Laden der Dateien!\")\n",
    "        return None\n",
    "    \n",
    "    # Erstellen einer Zuordnung von post_id zu qualitativen Bewertungen\n",
    "    print(\"Erstelle Mapping für qualitative Bewertungen...\")\n",
    "    qualitative_by_id = {}\n",
    "    for result in qualitative_data.get(\"results\", []):\n",
    "        post_id = result.get(\"post_id\")\n",
    "        if post_id:\n",
    "            qualitative_by_id[post_id] = result.get(\"evaluation\", {})\n",
    "    \n",
    "    # Ergebnis-Struktur erstellen\n",
    "    merged_data = {\n",
    "        \"individual_evaluations\": []\n",
    "    }\n",
    "    \n",
    "    # Zusammenführen der Daten\n",
    "    print(\"Führe Daten zusammen...\")\n",
    "    matched_count = 0\n",
    "    unmatched_count = 0\n",
    "    \n",
    "    for evaluation in technical_data.get(\"individual_evaluations\", []):\n",
    "        generated_id = evaluation.get(\"metadata\", {}).get(\"generated_id\")\n",
    "        qualitative_eval = qualitative_by_id.get(generated_id, {})\n",
    "        \n",
    "        # Prüfen, ob eine qualitative Bewertung gefunden wurde\n",
    "        if qualitative_eval:\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            unmatched_count += 1\n",
    "        \n",
    "        # Originale Struktur kopieren\n",
    "        merged_item = evaluation.copy()\n",
    "        \n",
    "        # LLM-Evaluation hinzufügen\n",
    "        merged_item[\"llm_evaluation\"] = qualitative_eval\n",
    "        \n",
    "        merged_data[\"individual_evaluations\"].append(merged_item)\n",
    "    \n",
    "    print(f\"Zusammenführung abgeschlossen:\")\n",
    "    print(f\"- Gefundene technische Bewertungen: {len(technical_data.get('individual_evaluations', []))}\")\n",
    "    print(f\"- Gefundene qualitative Bewertungen: {len(qualitative_data.get('results', []))}\")\n",
    "    print(f\"- Erfolgreiche Matches: {matched_count}\")\n",
    "    print(f\"- Einträge ohne Match: {unmatched_count}\")\n",
    "    \n",
    "    # Speichern der zusammengeführten Daten, falls gewünscht\n",
    "    if output_file:\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                json.dump(merged_data, file, indent=2, ensure_ascii=False)\n",
    "            print(f\"Zusammengeführte Daten wurden in {output_file} gespeichert.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Speichern der zusammengeführten Daten: {e}\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Zusammenführen der Dateien\n",
    "# \n",
    "# Gib hier die Pfade zu deinen JSON-Dateien an:\n",
    "\n",
    "# %%\n",
    "# Pfade zu den JSON-Dateien\n",
    "qualitative_file = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/judge/evaluation_results_20250325_121256.json\"  # Hier den Pfad anpassen\n",
    "technical_file   = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/eval_results.json\"  # Hier den Pfad anpassen\n",
    "output_file = \"/Users/christophhau/Desktop/Research_case/results/fine_tune03/merged_evaluations.json\"  # Name der Ausgabedatei\n",
    "\n",
    "# %%\n",
    "# Führe die Dateien zusammen\n",
    "merged_data = merge_json_files(technical_file, qualitative_file, output_file)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Analyse der zusammengeführten Daten\n",
    "# \n",
    "# Wir können nun einige Statistiken über die zusammengeführten Daten anzeigen:\n",
    "\n",
    "# %%\n",
    "if merged_data:\n",
    "    # Erstelle ein DataFrame mit einigen wichtigen Metriken\n",
    "    metrics_data = []\n",
    "    \n",
    "    for item in merged_data[\"individual_evaluations\"]:\n",
    "        metrics_row = {\n",
    "            \"generated_id\": item.get(\"metadata\", {}).get(\"generated_id\", \"\"),\n",
    "            \"rouge1_f1\": item.get(\"rouge_scores\", {}).get(\"rouge1\", {}).get(\"fmeasure\", 0),\n",
    "            \"rouge2_f1\": item.get(\"rouge_scores\", {}).get(\"rouge2\", {}).get(\"fmeasure\", 0),\n",
    "            \"rougeL_f1\": item.get(\"rouge_scores\", {}).get(\"rougeL\", {}).get(\"fmeasure\", 0),\n",
    "            \"similarity_score\": item.get(\"similarity_scores\", 0),\n",
    "            \"authenticity_score\": item.get(\"llm_evaluation\", {}).get(\"authenticity\", {}).get(\"score\", 0),\n",
    "            \"style_consistency_score\": item.get(\"llm_evaluation\", {}).get(\"style_consistency\", {}).get(\"score\", 0),\n",
    "            \"matching_intent\": item.get(\"llm_evaluation\", {}).get(\"matching_intent\", False)\n",
    "        }\n",
    "        metrics_data.append(metrics_row)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Zeige die ersten Einträge\n",
    "    print(\"Beispieleinträge aus den zusammengeführten Daten:\")\n",
    "    display(metrics_df.head())\n",
    "    \n",
    "    # Zeige einige Statistiken\n",
    "    print(\"\\nStatistiken zu den Metriken:\")\n",
    "    display(metrics_df.describe())\n",
    "    \n",
    "    # Zähle, wie oft intent übereinstimmt\n",
    "    intent_counts = metrics_df['matching_intent'].value_counts()\n",
    "    print(\"\\nIntent-Übereinstimmung:\")\n",
    "    display(intent_counts)\n",
    "else:\n",
    "    print(\"Keine Daten zum Analysieren verfügbar.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Beispiel für einen einzelnen Eintrag\n",
    "# \n",
    "# Schauen wir uns einen vollständigen Eintrag aus den zusammengeführten Daten an:\n",
    "\n",
    "# %%\n",
    "if merged_data and merged_data[\"individual_evaluations\"]:\n",
    "    sample_entry = merged_data[\"individual_evaluations\"][0]\n",
    "    print(json.dumps(sample_entry, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
